Text classification is a crucial task in natural language processing (NLP) with applications across diverse fields, 
including sentiment analysis, topic detection, and spam detection. This project aims to design and implement a 
comparative study on text classification using Large Language Models (LLMs), specifically BERT, GPT-4, and 
GPT-4 mini. Each of these models offers unique architectures and strengths, making them valuable candidates 
for benchmarking against various text classification tasks. The project will involve preprocessing and structuring 
text datasets, fine-tuning each model on these datasets, and evaluating their performance using standard metrics 
such as accuracy, F1-score, precision, and recall. The project aims to provide insights into the strengths and 
limitations of each model, particularly examining how well they generalize across different domains and dataset 
sizes. By comparing the resource requirements and performance of BERT, GPT-4, and GPT-4 mini, the study 
will contribute to the selection of appropriate models for specific text classification applications and support 
decisions in environments where computational efficiency is as critical as model accuracy. 
